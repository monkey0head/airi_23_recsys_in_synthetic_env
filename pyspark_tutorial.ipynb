{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7118a44",
   "metadata": {},
   "source": [
    "# Pyspark tutorial\n",
    "\n",
    "Симулятор для рекомендательных систем sim4rec использует pyspark и spark-датафреймы для работы с большими объемами данных. Данные в симуляторе хранятся в формате spark-датафреймов, поэтому будет полезно уметь с ними работать. Spark хранит данные партиционированно, по частям, и выполняет вычисления сначала, если возможно, внутри каждой партции, а затем уже выполняет shuffle, т.е. перемешивание данных, например, для group by и join. В ходе работы симулятора вы можете накопить большой объем данных, который будет долго и ресурсоемко полностью выгружать в привычный pandas. Поэтому советуем выполнять простые операции (фильтрацию, просмотр, джойны, группировки) в  pyspark, а затем, при необходимости конвертировать данные в pandas. \n",
    "\n",
    "* Spark сессия\n",
    "* Инициализация\n",
    "* Чтение/запись\n",
    "* Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74046b98",
   "metadata": {},
   "source": [
    "### Spark сессия\n",
    "Для начала нужно создать spark-сессию, в контексте которой будет создан DataFrame. В заданиях этот код написан за вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9700abfa-e1b2-4881-8954-f0dcb6231d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_PYTHON\"]=sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "755c5c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/12/01 10:42:18 WARN Utils: Your hostname, cl1nr5sb14mq6gk2g9m8-ilyr resolves to a loopback address: 127.0.1.1; using 10.129.0.35 instead (on interface eth0)\n",
      "22/12/01 10:42:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/venvs/hackenv/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "22/12/01 10:42:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/12/01 10:42:19 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.129.0.35:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySpark_Tutorial1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f99a8073f40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructField,\n",
    "    StructType,\n",
    "    IntegerType,\n",
    "    StringType\n",
    ")\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .master(\"local[*]\")\n",
    "    .appName('PySpark_Tutorial1')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30db1f01",
   "metadata": {},
   "source": [
    "### Инициализация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c735b",
   "metadata": {},
   "source": [
    "За создание объекта DataFrame отвечает метод `.createDataFrame()`, который в качестве данных может принимать:\n",
    "\n",
    "* pandas.DataFrame\n",
    "* dict\n",
    "* list\n",
    "* и т.д.\n",
    "\n",
    "При создании можно явно указать схему для DataFrame, например, это может помочь, когда вам нужно задать конкретные типы для колонок.\n",
    "\n",
    "Информация о датафрейме:\n",
    "* .show(n) - просмотр DataFrame\n",
    "* .count() - количество записей\n",
    "* .columns - список колонок\n",
    "* .printSchema() -  схема (колонки и типы)\n",
    "* .toPandas() - конвертация в pandas.DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c536a",
   "metadata": {},
   "source": [
    "#### Инициализация c помощью Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a17cb46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Nikita| 15|\n",
      "| Masha| 24|\n",
      "| Sasha| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_pd = pd.DataFrame({\"name\": [\"Nikita\", \"Masha\", \"Sasha\"], \"age\": [15, 24, 30]})\n",
    "df = spark.createDataFrame(data_pd)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fcfacbb",
   "metadata": {},
   "source": [
    "#### Конвертация в Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d90838b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nikita</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Masha</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sasha</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     name  age\n",
       "0  Nikita   15\n",
       "1   Masha   24\n",
       "2   Sasha   30"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ceb4c2",
   "metadata": {},
   "source": [
    "Методом `.show()` можно просмотреть определенное количество записей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "925df2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Nikita| 15|\n",
      "| Masha| 24|\n",
      "+------+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fa1480",
   "metadata": {},
   "source": [
    "#### Инициализация c помощью dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd1b63de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+\n",
      "|age|  name|\n",
      "+---+------+\n",
      "| 15|Nikita|\n",
      "| 24| Masha|\n",
      "| 30| Sasha|\n",
      "+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dct = [{\"name\": \"Nikita\", \"age\": 15}, \n",
    "           {\"name\": \"Masha\", \"age\": 24},\n",
    "           {\"name\": \"Sasha\", \"age\": 30}]\n",
    "df = spark.createDataFrame(data_dct)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0776f407",
   "metadata": {},
   "source": [
    "#### Инициализация c помощью list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6720657f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|    _1| _2|\n",
      "+------+---+\n",
      "|Nikita| 15|\n",
      "| Masha| 24|\n",
      "| Sasha| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"Nikita\", 15), (\"Masha\", 24), (\"Sasha\", 30)]\n",
    "df = spark.createDataFrame(data_list)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fd879e",
   "metadata": {},
   "source": [
    "#### Инициализация с использованием схемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "355f4afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Nikita| 15|\n",
      "| Masha| 24|\n",
      "| Sasha| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_list = [(\"Nikita\", 15), (\"Masha\", 24), (\"Sasha\", 30)]\n",
    "df = spark.createDataFrame(data_list, \n",
    "                           schema = StructType(\n",
    "                                [\n",
    "                                    StructField(\"name\", StringType()),\n",
    "                                    StructField(\"age\", IntegerType()),\n",
    "                                ]))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4f1d24",
   "metadata": {},
   "source": [
    "#### Методы просмотра информации о DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98f42a5a-cb44-4143-a97a-56f9fd2084dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3855bbdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "203fae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3c0014",
   "metadata": {},
   "source": [
    "### Чтение/запись\n",
    "\n",
    "Для **чтения** нужно обраться к модулю `spark.DataFrame.read`. <br>\n",
    "Для **записи** к модулю `spark.write`. <br>\n",
    "Так же для записи понадобиться метод `.mode()` с указанием следующих режимов сохранения:\n",
    "\n",
    "* overwrite - режим используется для перезаписи существующего файла.\n",
    "* append - Добавить данные в существующий файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13861fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Nikita| 15|\n",
      "| Masha| 24|\n",
      "| Sasha| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d60333",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").parquet(\"path_to_save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62493829-d85c-4abd-93c8-418b1bb5608b",
   "metadata": {},
   "source": [
    "pyspark сохраняет данные отдельных партиций в отдельные файлы. Данные в таком формате не получится прочитать pandas-ом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0165c0ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_SUCCESS\n",
      "part-00000-ca289f3c-9aa7-4b0c-b7a4-c68e44988eb9-c000.snappy.parquet\n",
      "part-00003-ca289f3c-9aa7-4b0c-b7a4-c68e44988eb9-c000.snappy.parquet\n",
      "part-00007-ca289f3c-9aa7-4b0c-b7a4-c68e44988eb9-c000.snappy.parquet\n",
      "part-00011-ca289f3c-9aa7-4b0c-b7a4-c68e44988eb9-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "!cd path_to_save && ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4399bef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = spark.read.parquet(\"path_to_save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b90fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  name|age|\n",
      "+------+---+\n",
      "|Nikita| 15|\n",
      "| Masha| 24|\n",
      "| Sasha| 30|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b14531",
   "metadata": {},
   "source": [
    "### Spark SQL\n",
    "\n",
    "Для запросов в Spark используется SQL-like синтаксис.\n",
    "\n",
    "Основные методы:\n",
    "* .select() - просмотр столбцов\n",
    "* .filter(), .where() - фильтрация записей\n",
    "* .join() - объединение нескольких spark.DataFrame\n",
    "* .distinct() - уникальные значения\n",
    "* .withColumn() - создание/преобразование столбца\n",
    "* .withColumnRename() - переименование столбца\n",
    "* .orderBy() - сортировка\n",
    "* .groupBy().agg() - группировка\n",
    "* и т.д.\n",
    "\n",
    "Существуют несколько способов обращения к столбцу:\n",
    "\n",
    "* строка - \"column_name\"\n",
    "* модуль pyspark.sql.functions - sf.col(\"column_name\")\n",
    "* поле объекта - df.column_name\n",
    "\n",
    "Помимо метода `.withColumnRename()`, вызываемого от DataFrame  может использоваться метод  `.alias()`, вызываемый от определенного столбца.\n",
    "\n",
    "Модуль pyspark.sql.functions используется для преобразования данных в столбцах с помощью функций. [Список функций](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "\n",
    "Основные методы:\n",
    "* .col() - без преобразований\n",
    "* .max(), .min(), .mean(), .count()  - арифметические функции\n",
    "* .lit() - столбец-константа\n",
    "* .when().otherwise() - условное выражение\n",
    "* и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb467cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+\n",
      "|age|height|  name|\n",
      "+---+------+------+\n",
      "| 20|   170|Nikita|\n",
      "| 21|   180| Masha|\n",
      "| 20|   170| Sasha|\n",
      "| 30|   175|  Lera|\n",
      "| 30|   170|  Vika|\n",
      "| 21|   175|   Max|\n",
      "| 30|   175| Misha|\n",
      "+---+------+------+\n",
      "\n",
      "+-------------+------+\n",
      "|         city|  name|\n",
      "+-------------+------+\n",
      "|St. Peterburg|Nikita|\n",
      "|      Moscoow| Sasha|\n",
      "|St. Peterburg|  Vika|\n",
      "|St. Peterburg| Tanya|\n",
      "|      Moscoow| Misha|\n",
      "+-------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as sf\n",
    "\n",
    "data_dct_info = [{\"name\": \"Nikita\", \"age\": 20, \"height\": 170}, \n",
    "              {\"name\": \"Masha\", \"age\": 21, \"height\": 180},\n",
    "              {\"name\": \"Sasha\", \"age\": 20, \"height\": 170},\n",
    "              {\"name\": \"Lera\", \"age\": 30, \"height\": 175},\n",
    "              {\"name\": \"Vika\", \"age\": 30, \"height\": 170},\n",
    "              {\"name\": \"Max\", \"age\": 21, \"height\": 175},\n",
    "              {\"name\": \"Misha\", \"age\": 30, \"height\": 175}]\n",
    "\n",
    "data_dct_city = [{\"name\": \"Nikita\", \"city\": \"St. Peterburg\"}, \n",
    "              {\"name\": \"Sasha\", \"city\": \"Moscoow\"},\n",
    "              {\"name\": \"Vika\", \"city\": \"St. Peterburg\"},\n",
    "              {\"name\": \"Tanya\", \"city\": \"St. Peterburg\"},\n",
    "              {\"name\": \"Misha\", \"city\": \"Moscoow\"}]\n",
    "\n",
    "names_info = spark.createDataFrame(data_dct_info)\n",
    "\n",
    "names_city = spark.createDataFrame(data_dct_city)\n",
    "\n",
    "names_info.show()\n",
    "names_city.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900932ff",
   "metadata": {},
   "source": [
    "**Разные способы обращения к столбцу**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8977c7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   170|\n",
      "|   180|\n",
      "|   170|\n",
      "|   175|\n",
      "|   170|\n",
      "|   175|\n",
      "|   175|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.select(\"height\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f0a582d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   170|\n",
      "|   180|\n",
      "|   170|\n",
      "|   175|\n",
      "|   170|\n",
      "|   175|\n",
      "|   175|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.select(sf.col(\"height\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db0f238",
   "metadata": {},
   "source": [
    "#### Основные методы  Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498e39e",
   "metadata": {},
   "source": [
    "Фильтрация по одному столбцу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566c362b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 30|   175| Lera|\n",
      "| 30|   170| Vika|\n",
      "| 30|   175|Misha|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.filter(\"age > 25\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6240cc",
   "metadata": {},
   "source": [
    "Фильтрация по нескольким столбцам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d4abeab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+\n",
      "|age|height| name|\n",
      "+---+------+-----+\n",
      "| 30|   175| Lera|\n",
      "| 30|   175|Misha|\n",
      "+---+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.filter(\"age > 25 and height > 170\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80bd28d1",
   "metadata": {},
   "source": [
    "Уникальные значения столбца "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "afa8c6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|height|\n",
      "+------+\n",
      "|   170|\n",
      "|   175|\n",
      "|   180|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.select(\"height\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0159e0a5",
   "metadata": {},
   "source": [
    "С использованием `pyspark.sql.functions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1bb8608f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(height)|\n",
      "+-----------+\n",
      "|        180|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.select(sf.max(\"height\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8684359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|       mean height|\n",
      "+------------------+\n",
      "|173.57142857142858|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.select(sf.mean(\"height\").alias(\"mean height\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e44b2a-767f-483e-89c2-3e2860b620a0",
   "metadata": {},
   "source": [
    "##### Сложные преобразования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c69bc94d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|age_over_25|\n",
      "+---+-----------+\n",
      "| 20|      false|\n",
      "| 20|      false|\n",
      "| 21|      false|\n",
      "| 21|      false|\n",
      "| 30|       true|\n",
      "| 30|       true|\n",
      "| 30|       true|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    names_info\n",
    "    .select(\"age\")\n",
    "    .withColumn(\"age_over_25\", sf.when(sf.col(\"age\") > 25, True).otherwise(False))\n",
    "    .orderBy(\"age\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "05f3317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|age|age_over_25|\n",
      "+---+-----------+\n",
      "| 20|      false|\n",
      "| 20|      false|\n",
      "| 21|      false|\n",
      "| 21|      false|\n",
      "| 30|       true|\n",
      "| 30|       true|\n",
      "| 30|       true|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(\n",
    "    names_info.\n",
    "    select(\"age\")\n",
    "    .filter(\"age > 15\")\n",
    "    .withColumn(\"age_over_25\", sf.col(\"age\") > 25)\n",
    "    .orderBy(\"age\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3c4667",
   "metadata": {},
   "source": [
    "#### Join датафреймов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0625c4",
   "metadata": {},
   "source": [
    "Inner join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "28f327ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+-------------+\n",
      "|  name|age|height|         city|\n",
      "+------+---+------+-------------+\n",
      "|  Vika| 30|   170|St. Peterburg|\n",
      "| Sasha| 20|   170|      Moscoow|\n",
      "| Misha| 30|   175|      Moscoow|\n",
      "|Nikita| 20|   170|St. Peterburg|\n",
      "+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.join(names_city, on=\"name\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f7aebd",
   "metadata": {},
   "source": [
    "Left join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d13233ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+------+-------------+\n",
      "|  name|age|height|         city|\n",
      "+------+---+------+-------------+\n",
      "|  Vika| 30|   170|St. Peterburg|\n",
      "| Sasha| 20|   170|      Moscoow|\n",
      "|  Lera| 30|   175|         null|\n",
      "| Misha| 30|   175|      Moscoow|\n",
      "|Nikita| 20|   170|St. Peterburg|\n",
      "|   Max| 21|   175|         null|\n",
      "| Masha| 21|   180|         null|\n",
      "+------+---+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.join(names_city, on=\"name\", how=\"left\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652ac77e",
   "metadata": {},
   "source": [
    " Right join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c6a90182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------+-------------+\n",
      "|  name| age|height|         city|\n",
      "+------+----+------+-------------+\n",
      "|  Vika|  30|   170|St. Peterburg|\n",
      "| Sasha|  20|   170|      Moscoow|\n",
      "| Tanya|null|  null|St. Peterburg|\n",
      "| Misha|  30|   175|      Moscoow|\n",
      "|Nikita|  20|   170|St. Peterburg|\n",
      "+------+----+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_info.join(names_city, on=\"name\", how=\"right\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4078a5ed",
   "metadata": {},
   "source": [
    "Комбинация join и запросов к DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a04d3fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+----+------+\n",
      "|  name|         city| age|height|\n",
      "+------+-------------+----+------+\n",
      "|  Vika|St. Peterburg|null|  null|\n",
      "| Sasha|      Moscoow|null|  null|\n",
      "| Tanya|St. Peterburg|null|  null|\n",
      "| Misha|      Moscoow|  30|   175|\n",
      "|Nikita|St. Peterburg|null|  null|\n",
      "+------+-------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names_city.join(\n",
    "    names_info.where(\n",
    "        sf.col(\"height\") > 170\n",
    "    ),\n",
    "    on=\"name\",\n",
    "    how=\"left\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810da760-6f5b-4b4a-be21-d5887444b737",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackenv",
   "language": "python",
   "name": "hackenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
